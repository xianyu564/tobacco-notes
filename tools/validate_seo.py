#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SEOå’Œç»“æ„åŒ–æ•°æ®éªŒè¯å·¥å…·
SEO and Structured Data Validation Tool
"""

import json
import re
import requests
from pathlib import Path
from urllib.parse import urljoin, urlparse
import xml.etree.ElementTree as ET
from datetime import datetime
from typing import Dict, List, Optional


class SEOValidator:
    """SEOéªŒè¯å™¨"""
    
    def __init__(self, base_url: str = "https://xianyu564.github.io/tobacco-notes", repo_root: Path = None):
        self.base_url = base_url.rstrip('/')
        self.repo_root = repo_root or Path(__file__).resolve().parents[1]
        self.docs_dir = self.repo_root / 'docs'
        self.errors = []
        self.warnings = []
        self.suggestions = []
        
        # SEOéªŒè¯è§„åˆ™
        self.seo_rules = {
            'title_length': (30, 60),
            'description_length': (120, 160),
            'h1_count': (1, 1),
            'alt_text_length': (5, 125),
            'url_length': (0, 100),
            'keyword_density': (0.01, 0.03)
        }
    
    def validate_all_seo(self) -> Dict:
        """éªŒè¯æ‰€æœ‰SEOè¦ç´ """
        print("ğŸš€ å¼€å§‹SEOç»¼åˆéªŒè¯...")
        print("=" * 50)
        
        # éªŒè¯é¡µé¢SEO
        self._validate_page_seo()
        
        # éªŒè¯ç»“æ„åŒ–æ•°æ®
        self._validate_structured_data()
        
        # éªŒè¯ç«™ç‚¹åœ°å›¾
        self._validate_sitemap()
        
        # éªŒè¯robots.txt
        self._validate_robots()
        
        # éªŒè¯ç¤¾äº¤åª’ä½“å…ƒæ•°æ®
        self._validate_social_meta()
        
        # éªŒè¯æŠ€æœ¯SEO
        self._validate_technical_seo()
        
        # ç”ŸæˆæŠ¥å‘Š
        return self._generate_seo_report()
    
    def _validate_page_seo(self):
        """éªŒè¯é¡µé¢SEOå…ƒç´ """
        print("\nğŸ“„ éªŒè¯é¡µé¢SEO...")
        
        index_file = self.docs_dir / 'index.html'
        if not index_file.exists():
            self.errors.append("ä¸»é¡µæ–‡ä»¶ä¸å­˜åœ¨")
            return
        
        try:
            with open(index_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # éªŒè¯titleæ ‡ç­¾
            title_match = re.search(r'<title[^>]*>(.*?)</title>', content, re.IGNORECASE | re.DOTALL)
            if title_match:
                title = title_match.group(1).strip()
                title_length = len(title)
                min_len, max_len = self.seo_rules['title_length']
                
                if title_length < min_len:
                    self.warnings.append(f"é¡µé¢æ ‡é¢˜è¿‡çŸ­ ({title_length}å­—ç¬¦ï¼Œå»ºè®®{min_len}-{max_len}å­—ç¬¦)")
                elif title_length > max_len:
                    self.warnings.append(f"é¡µé¢æ ‡é¢˜è¿‡é•¿ ({title_length}å­—ç¬¦ï¼Œå»ºè®®{min_len}-{max_len}å­—ç¬¦)")
                else:
                    print(f"âœ… é¡µé¢æ ‡é¢˜é•¿åº¦åˆé€‚ ({title_length}å­—ç¬¦)")
            else:
                self.errors.append("ç¼ºå°‘é¡µé¢æ ‡é¢˜")
            
            # éªŒè¯meta description
            desc_match = re.search(r'<meta[^>]*name=["\']description["\'][^>]*content=["\']([^"\']*)["\']', content, re.IGNORECASE)
            if desc_match:
                description = desc_match.group(1).strip()
                desc_length = len(description)
                min_len, max_len = self.seo_rules['description_length']
                
                if desc_length < min_len:
                    self.warnings.append(f"é¡µé¢æè¿°è¿‡çŸ­ ({desc_length}å­—ç¬¦ï¼Œå»ºè®®{min_len}-{max_len}å­—ç¬¦)")
                elif desc_length > max_len:
                    self.warnings.append(f"é¡µé¢æè¿°è¿‡é•¿ ({desc_length}å­—ç¬¦ï¼Œå»ºè®®{min_len}-{max_len}å­—ç¬¦)")
                else:
                    print(f"âœ… é¡µé¢æè¿°é•¿åº¦åˆé€‚ ({desc_length}å­—ç¬¦)")
            else:
                self.errors.append("ç¼ºå°‘meta description")
            
            # éªŒè¯H1æ ‡ç­¾
            h1_matches = re.findall(r'<h1[^>]*>(.*?)</h1>', content, re.IGNORECASE | re.DOTALL)
            h1_count = len(h1_matches)
            
            if h1_count == 0:
                self.errors.append("ç¼ºå°‘H1æ ‡ç­¾")
            elif h1_count > 1:
                self.warnings.append(f"å­˜åœ¨å¤šä¸ªH1æ ‡ç­¾ ({h1_count}ä¸ªï¼Œå»ºè®®åªæœ‰1ä¸ª)")
            else:
                print("âœ… H1æ ‡ç­¾æ•°é‡åˆé€‚")
            
            # éªŒè¯è¯­è¨€å£°æ˜
            if 'lang=' not in content:
                self.warnings.append("ç¼ºå°‘é¡µé¢è¯­è¨€å£°æ˜")
            
            # éªŒè¯viewportå…ƒæ ‡ç­¾
            if 'viewport' not in content:
                self.warnings.append("ç¼ºå°‘viewportå…ƒæ ‡ç­¾")
            
            # éªŒè¯å­—ç¬¦ç¼–ç 
            if 'charset=' not in content:
                self.warnings.append("ç¼ºå°‘å­—ç¬¦ç¼–ç å£°æ˜")
                
        except Exception as e:
            self.errors.append(f"è¯»å–ä¸»é¡µæ–‡ä»¶å¤±è´¥: {str(e)}")
    
    def _validate_structured_data(self):
        """éªŒè¯ç»“æ„åŒ–æ•°æ®"""
        print("\nğŸ—ï¸  éªŒè¯ç»“æ„åŒ–æ•°æ®...")
        
        index_file = self.docs_dir / 'index.html'
        if not index_file.exists():
            return
        
        try:
            with open(index_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # æ£€æŸ¥JSON-LDç»“æ„åŒ–æ•°æ®
            jsonld_pattern = r'<script[^>]*type=["\']application/ld\+json["\'][^>]*>(.*?)</script>'
            jsonld_matches = re.findall(jsonld_pattern, content, re.IGNORECASE | re.DOTALL)
            
            if jsonld_matches:
                for i, jsonld_content in enumerate(jsonld_matches):
                    try:
                        data = json.loads(jsonld_content.strip())
                        self._validate_jsonld_schema(data, i)
                    except json.JSONDecodeError as e:
                        self.errors.append(f"JSON-LDæ ¼å¼é”™è¯¯ [{i}]: {str(e)}")
            else:
                self.suggestions.append("å»ºè®®æ·»åŠ JSON-LDç»“æ„åŒ–æ•°æ®ä»¥æ”¹å–„SEO")
            
            # æ£€æŸ¥å¾®æ•°æ®
            if 'itemscope' in content:
                print("âœ… æ£€æµ‹åˆ°å¾®æ•°æ®æ ‡è®°")
            
            # æ£€æŸ¥Open Graphæ•°æ®
            og_patterns = [
                r'<meta[^>]*property=["\']og:title["\']',
                r'<meta[^>]*property=["\']og:description["\']',
                r'<meta[^>]*property=["\']og:image["\']',
                r'<meta[^>]*property=["\']og:url["\']'
            ]
            
            og_count = sum(1 for pattern in og_patterns if re.search(pattern, content, re.IGNORECASE))
            if og_count == len(og_patterns):
                print("âœ… Open Graphå…ƒæ•°æ®å®Œæ•´")
            elif og_count > 0:
                self.warnings.append(f"Open Graphå…ƒæ•°æ®ä¸å®Œæ•´ ({og_count}/{len(og_patterns)}ä¸ª)")
            else:
                self.suggestions.append("å»ºè®®æ·»åŠ Open Graphå…ƒæ•°æ®")
            
            # æ£€æŸ¥Twitter Cardæ•°æ®
            twitter_patterns = [
                r'<meta[^>]*name=["\']twitter:card["\']',
                r'<meta[^>]*name=["\']twitter:title["\']',
                r'<meta[^>]*name=["\']twitter:description["\']'
            ]
            
            twitter_count = sum(1 for pattern in twitter_patterns if re.search(pattern, content, re.IGNORECASE))
            if twitter_count == len(twitter_patterns):
                print("âœ… Twitter Cardå…ƒæ•°æ®å®Œæ•´")
            elif twitter_count > 0:
                self.warnings.append(f"Twitter Cardå…ƒæ•°æ®ä¸å®Œæ•´ ({twitter_count}/{len(twitter_patterns)}ä¸ª)")
            else:
                self.suggestions.append("å»ºè®®æ·»åŠ Twitter Cardå…ƒæ•°æ®")
                
        except Exception as e:
            self.errors.append(f"éªŒè¯ç»“æ„åŒ–æ•°æ®å¤±è´¥: {str(e)}")
    
    def _validate_jsonld_schema(self, data: Dict, index: int):
        """éªŒè¯JSON-LDæ¨¡å¼"""
        if isinstance(data, dict):
            # éªŒè¯@context
            if '@context' not in data:
                self.warnings.append(f"JSON-LDç¼ºå°‘@context [{index}]")
            
            # éªŒè¯@type
            if '@type' not in data:
                self.warnings.append(f"JSON-LDç¼ºå°‘@type [{index}]")
            else:
                schema_type = data['@type']
                print(f"âœ… æ£€æµ‹åˆ°Schema.orgç±»å‹: {schema_type}")
            
            # é’ˆå¯¹ä¸åŒç±»å‹çš„ç‰¹å®šéªŒè¯
            if data.get('@type') == 'WebSite':
                self._validate_website_schema(data, index)
            elif data.get('@type') == 'Article':
                self._validate_article_schema(data, index)
            elif data.get('@type') == 'Review':
                self._validate_review_schema(data, index)
    
    def _validate_website_schema(self, data: Dict, index: int):
        """éªŒè¯Websiteæ¨¡å¼"""
        required_fields = ['name', 'url']
        for field in required_fields:
            if field not in data:
                self.warnings.append(f"Websiteæ¨¡å¼ç¼ºå°‘{field}å­—æ®µ [{index}]")
        
        # éªŒè¯searchAction
        if 'potentialAction' in data:
            action = data['potentialAction']
            if isinstance(action, dict) and action.get('@type') == 'SearchAction':
                if 'target' not in action:
                    self.warnings.append(f"SearchActionç¼ºå°‘targetå­—æ®µ [{index}]")
                if 'query-input' not in action:
                    self.warnings.append(f"SearchActionç¼ºå°‘query-inputå­—æ®µ [{index}]")
    
    def _validate_article_schema(self, data: Dict, index: int):
        """éªŒè¯Articleæ¨¡å¼"""
        required_fields = ['headline', 'author', 'datePublished']
        for field in required_fields:
            if field not in data:
                self.warnings.append(f"Articleæ¨¡å¼ç¼ºå°‘{field}å­—æ®µ [{index}]")
        
        # éªŒè¯å›¾ç‰‡
        if 'image' not in data:
            self.suggestions.append(f"Articleæ¨¡å¼å»ºè®®æ·»åŠ imageå­—æ®µ [{index}]")
    
    def _validate_review_schema(self, data: Dict, index: int):
        """éªŒè¯Reviewæ¨¡å¼"""
        required_fields = ['itemReviewed', 'reviewRating', 'author']
        for field in required_fields:
            if field not in data:
                self.warnings.append(f"Reviewæ¨¡å¼ç¼ºå°‘{field}å­—æ®µ [{index}]")
        
        # éªŒè¯è¯„åˆ†
        if 'reviewRating' in data:
            rating = data['reviewRating']
            if isinstance(rating, dict):
                if 'ratingValue' not in rating:
                    self.warnings.append(f"è¯„åˆ†ç¼ºå°‘ratingValue [{index}]")
                if 'bestRating' not in rating:
                    self.suggestions.append(f"è¯„åˆ†å»ºè®®æ·»åŠ bestRating [{index}]")
    
    def _validate_sitemap(self):
        """éªŒè¯ç«™ç‚¹åœ°å›¾"""
        print("\nğŸ—ºï¸  éªŒè¯ç«™ç‚¹åœ°å›¾...")
        
        sitemap_file = self.docs_dir / 'sitemap.xml'
        if not sitemap_file.exists():
            self.warnings.append("ç«™ç‚¹åœ°å›¾ä¸å­˜åœ¨")
            return
        
        try:
            tree = ET.parse(sitemap_file)
            root = tree.getroot()
            
            # æ£€æŸ¥å‘½åç©ºé—´
            if 'http://www.sitemaps.org/schemas/sitemap/0.9' not in root.tag:
                self.warnings.append("ç«™ç‚¹åœ°å›¾å‘½åç©ºé—´ä¸æ­£ç¡®")
            
            # æ£€æŸ¥URLæ•°é‡
            urls = root.findall('.//{http://www.sitemaps.org/schemas/sitemap/0.9}url')
            url_count = len(urls)
            
            if url_count == 0:
                self.errors.append("ç«™ç‚¹åœ°å›¾ä¸­æ²¡æœ‰URL")
            elif url_count > 50000:
                self.warnings.append(f"ç«™ç‚¹åœ°å›¾URLè¿‡å¤š ({url_count})ï¼Œå»ºè®®åˆ†å‰²")
            else:
                print(f"âœ… ç«™ç‚¹åœ°å›¾åŒ…å«{url_count}ä¸ªURL")
            
            # éªŒè¯URLæ ¼å¼
            for i, url in enumerate(urls[:10]):  # åªæ£€æŸ¥å‰10ä¸ª
                loc = url.find('.//{http://www.sitemaps.org/schemas/sitemap/0.9}loc')
                if loc is not None:
                    url_text = loc.text
                    if not url_text.startswith(('http://', 'https://')):
                        self.errors.append(f"ç«™ç‚¹åœ°å›¾åŒ…å«æ— æ•ˆURL: {url_text}")
                    
                    # æ£€æŸ¥lastmodæ ¼å¼
                    lastmod = url.find('.//{http://www.sitemaps.org/schemas/sitemap/0.9}lastmod')
                    if lastmod is not None:
                        lastmod_text = lastmod.text
                        try:
                            datetime.fromisoformat(lastmod_text.replace('Z', '+00:00'))
                        except ValueError:
                            self.warnings.append(f"lastmodæ—¥æœŸæ ¼å¼ä¸æ­£ç¡®: {lastmod_text}")
                            
        except Exception as e:
            self.errors.append(f"éªŒè¯ç«™ç‚¹åœ°å›¾å¤±è´¥: {str(e)}")
    
    def _validate_robots(self):
        """éªŒè¯robots.txt"""
        print("\nğŸ¤– éªŒè¯robots.txt...")
        
        robots_file = self.docs_dir / 'robots.txt'
        if not robots_file.exists():
            self.warnings.append("robots.txtæ–‡ä»¶ä¸å­˜åœ¨")
            return
        
        try:
            with open(robots_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            lines = content.strip().split('\n')
            
            # æ£€æŸ¥User-agentå£°æ˜
            if not any(line.strip().startswith('User-agent:') for line in lines):
                self.errors.append("robots.txtç¼ºå°‘User-agentå£°æ˜")
            
            # æ£€æŸ¥SitemapæŒ‡å‘
            sitemap_lines = [line for line in lines if line.strip().startswith('Sitemap:')]
            if not sitemap_lines:
                self.warnings.append("robots.txtç¼ºå°‘SitemapæŒ‡å‘")
            else:
                for line in sitemap_lines:
                    sitemap_url = line.split(':', 1)[1].strip()
                    if not sitemap_url.startswith(('http://', 'https://')):
                        self.warnings.append(f"Sitemap URLæ ¼å¼ä¸æ­£ç¡®: {sitemap_url}")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰è¿‡åº¦é™åˆ¶
            disallow_all = any(line.strip() == 'Disallow: /' for line in lines)
            if disallow_all:
                self.warnings.append("robots.txtç¦æ­¢äº†æ‰€æœ‰çˆ¬è™«è®¿é—®")
            
            print("âœ… robots.txtåŸºæœ¬æ ¼å¼æ­£ç¡®")
            
        except Exception as e:
            self.errors.append(f"éªŒè¯robots.txtå¤±è´¥: {str(e)}")
    
    def _validate_social_meta(self):
        """éªŒè¯ç¤¾äº¤åª’ä½“å…ƒæ•°æ®"""
        print("\nğŸ“± éªŒè¯ç¤¾äº¤åª’ä½“å…ƒæ•°æ®...")
        
        index_file = self.docs_dir / 'index.html'
        if not index_file.exists():
            return
        
        try:
            with open(index_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # æ£€æŸ¥favicon
            favicon_patterns = [
                r'<link[^>]*rel=["\']icon["\']',
                r'<link[^>]*rel=["\']shortcut icon["\']',
                r'<link[^>]*rel=["\']apple-touch-icon["\']'
            ]
            
            favicon_count = sum(1 for pattern in favicon_patterns if re.search(pattern, content, re.IGNORECASE))
            if favicon_count > 0:
                print(f"âœ… æ£€æµ‹åˆ°favicon ({favicon_count}ä¸ª)")
            else:
                self.warnings.append("ç¼ºå°‘favicon")
            
            # æ£€æŸ¥ä¸»é¢˜è‰²
            theme_color_pattern = r'<meta[^>]*name=["\']theme-color["\']'
            if re.search(theme_color_pattern, content, re.IGNORECASE):
                print("âœ… è®¾ç½®äº†ä¸»é¢˜è‰²")
            else:
                self.suggestions.append("å»ºè®®è®¾ç½®theme-color")
            
            # æ£€æŸ¥manifest
            manifest_pattern = r'<link[^>]*rel=["\']manifest["\']'
            if re.search(manifest_pattern, content, re.IGNORECASE):
                print("âœ… æ£€æµ‹åˆ°Web App Manifest")
            else:
                self.suggestions.append("å»ºè®®æ·»åŠ Web App Manifest")
                
        except Exception as e:
            self.errors.append(f"éªŒè¯ç¤¾äº¤åª’ä½“å…ƒæ•°æ®å¤±è´¥: {str(e)}")
    
    def _validate_technical_seo(self):
        """éªŒè¯æŠ€æœ¯SEO"""
        print("\nâš™ï¸  éªŒè¯æŠ€æœ¯SEO...")
        
        # æ£€æŸ¥HTTPSé‡å®šå‘é…ç½®
        htaccess_file = self.docs_dir / '.htaccess'
        if htaccess_file.exists():
            try:
                with open(htaccess_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # æ£€æŸ¥HTTPSé‡å®šå‘
                if 'https' in content.lower() or 'ssl' in content.lower():
                    print("âœ… æ£€æµ‹åˆ°HTTPSç›¸å…³é…ç½®")
                else:
                    self.suggestions.append("å»ºè®®é…ç½®HTTPSé‡å®šå‘")
                
                # æ£€æŸ¥å‹ç¼©é…ç½®
                if 'gzip' in content.lower() or 'deflate' in content.lower():
                    print("âœ… æ£€æµ‹åˆ°å‹ç¼©é…ç½®")
                else:
                    self.suggestions.append("å»ºè®®å¯ç”¨Gzipå‹ç¼©")
                
                # æ£€æŸ¥ç¼“å­˜é…ç½®
                if 'expires' in content.lower() or 'cache-control' in content.lower():
                    print("âœ… æ£€æµ‹åˆ°ç¼“å­˜é…ç½®")
                else:
                    self.suggestions.append("å»ºè®®é…ç½®æµè§ˆå™¨ç¼“å­˜")
                    
            except Exception as e:
                self.warnings.append(f"è¯»å–.htaccesså¤±è´¥: {str(e)}")
        
        # æ£€æŸ¥é¡µé¢åŠ è½½é€Ÿåº¦ç›¸å…³æ–‡ä»¶
        critical_files = ['styles.css', 'index.html']
        for filename in critical_files:
            file_path = self.docs_dir / filename
            if file_path.exists():
                file_size = file_path.stat().st_size
                if filename.endswith('.css') and file_size > 50 * 1024:
                    self.warnings.append(f"CSSæ–‡ä»¶è¾ƒå¤§: {filename} ({file_size // 1024}KB)")
                elif filename.endswith('.js') and file_size > 100 * 1024:
                    self.warnings.append(f"JavaScriptæ–‡ä»¶è¾ƒå¤§: {filename} ({file_size // 1024}KB)")
    
    def _generate_seo_report(self) -> Dict:
        """ç”ŸæˆSEOæŠ¥å‘Š"""
        total_issues = len(self.errors) + len(self.warnings)
        max_score = 100
        
        if total_issues == 0:
            seo_score = max_score
        else:
            # é”™è¯¯æƒé‡æ›´é«˜
            error_weight = 15
            warning_weight = 5
            
            penalty = (len(self.errors) * error_weight) + (len(self.warnings) * warning_weight)
            seo_score = max(0, max_score - penalty)
        
        report = {
            'timestamp': datetime.now().isoformat(),
            'score': seo_score,
            'summary': {
                'errors': len(self.errors),
                'warnings': len(self.warnings),
                'suggestions': len(self.suggestions)
            },
            'details': {
                'errors': self.errors,
                'warnings': self.warnings,
                'suggestions': self.suggestions
            },
            'recommendations': self._generate_seo_recommendations()
        }
        
        self._log_seo_report(report)
        return report
    
    def _generate_seo_recommendations(self) -> List[Dict]:
        """ç”ŸæˆSEOå»ºè®®"""
        recommendations = []
        
        if any('æ ‡é¢˜' in error for error in self.errors + self.warnings):
            recommendations.append({
                'priority': 'high',
                'category': 'é¡µé¢æ ‡é¢˜',
                'suggestion': 'ä¼˜åŒ–é¡µé¢æ ‡é¢˜ï¼šç¡®ä¿é•¿åº¦é€‚ä¸­(30-60å­—ç¬¦)ï¼ŒåŒ…å«å…³é”®è¯ï¼Œæè¿°å‡†ç¡®'
            })
        
        if any('description' in error.lower() for error in self.errors + self.warnings):
            recommendations.append({
                'priority': 'high',
                'category': 'Metaæè¿°',
                'suggestion': 'ä¼˜åŒ–Metaæè¿°ï¼šé•¿åº¦120-160å­—ç¬¦ï¼ŒåŒ…å«å…³é”®è¯ï¼Œå¸å¼•ç‚¹å‡»'
            })
        
        if any('ç»“æ„åŒ–æ•°æ®' in suggestion for suggestion in self.suggestions):
            recommendations.append({
                'priority': 'medium',
                'category': 'ç»“æ„åŒ–æ•°æ®',
                'suggestion': 'æ·»åŠ JSON-LDç»“æ„åŒ–æ•°æ®ï¼šæå‡æœç´¢å¼•æ“ç†è§£ï¼Œè·å¾—å¯Œæ‘˜è¦'
            })
        
        if any('Open Graph' in warning for warning in self.warnings):
            recommendations.append({
                'priority': 'medium',
                'category': 'ç¤¾äº¤åª’ä½“',
                'suggestion': 'å®Œå–„Open Graphå’ŒTwitter Cardå…ƒæ•°æ®ï¼šæ”¹å–„ç¤¾äº¤åˆ†äº«æ•ˆæœ'
            })
        
        if any('å‹ç¼©' in suggestion for suggestion in self.suggestions):
            recommendations.append({
                'priority': 'low',
                'category': 'æ€§èƒ½ä¼˜åŒ–',
                'suggestion': 'å¯ç”¨å‹ç¼©å’Œç¼“å­˜ï¼šæå‡é¡µé¢åŠ è½½é€Ÿåº¦ï¼Œæ”¹å–„ç”¨æˆ·ä½“éªŒ'
            })
        
        return recommendations
    
    def _log_seo_report(self, report: Dict):
        """è¾“å‡ºSEOæŠ¥å‘Š"""
        print("\n" + "=" * 50)
        print("ğŸ“Š SEOéªŒè¯æŠ¥å‘Š")
        print("=" * 50)
        
        print(f"ğŸ¯ SEOå¾—åˆ†: {report['score']}/100")
        print(f"âŒ é”™è¯¯: {report['summary']['errors']}")
        print(f"âš ï¸ è­¦å‘Š: {report['summary']['warnings']}")
        print(f"ğŸ’¡ å»ºè®®: {report['summary']['suggestions']}")
        
        if self.errors:
            print(f"\nâŒ é”™è¯¯ ({len(self.errors)}):")
            for error in self.errors[:10]:
                print(f"   â€¢ {error}")
            if len(self.errors) > 10:
                print(f"   ... è¿˜æœ‰ {len(self.errors) - 10} ä¸ªé”™è¯¯")
        
        if self.warnings:
            print(f"\nâš ï¸ è­¦å‘Š ({len(self.warnings)}):")
            for warning in self.warnings[:10]:
                print(f"   â€¢ {warning}")
            if len(self.warnings) > 10:
                print(f"   ... è¿˜æœ‰ {len(self.warnings) - 10} ä¸ªè­¦å‘Š")
        
        if self.suggestions:
            print(f"\nğŸ’¡ å»ºè®® ({len(self.suggestions)}):")
            for suggestion in self.suggestions[:10]:
                print(f"   â€¢ {suggestion}")
            if len(self.suggestions) > 10:
                print(f"   ... è¿˜æœ‰ {len(self.suggestions) - 10} ä¸ªå»ºè®®")
        
        if report['recommendations']:
            print("\nğŸ¯ ä¼˜åŒ–å»ºè®®:")
            for rec in report['recommendations']:
                priority_icon = {'high': 'ğŸ”´', 'medium': 'ğŸŸ¡', 'low': 'ğŸŸ¢'}.get(rec['priority'], 'âšª')
                print(f"   {priority_icon} {rec['category']}: {rec['suggestion']}")
        
        if report['score'] >= 90:
            print("\nğŸ‰ SEOä¼˜åŒ–ä¼˜ç§€ï¼")
        elif report['score'] >= 70:
            print("\nâœ… SEOä¼˜åŒ–è‰¯å¥½ï¼Œè¿˜æœ‰æ”¹è¿›ç©ºé—´")
        else:
            print("\nâš ï¸ SEOéœ€è¦æ˜¾è‘—æ”¹è¿›")


def main():
    """ä¸»å‡½æ•°"""
    seo_validator = SEOValidator()
    result = seo_validator.validate_all_seo()
    
    # ä¿å­˜SEOæŠ¥å‘Š
    report_file = seo_validator.repo_root / 'docs' / 'data' / 'seo-report.json'
    report_file.parent.mkdir(exist_ok=True)
    
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(result, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ“„ SEOæŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
    
    return result['score'] >= 70


if __name__ == '__main__':
    import sys
    success = main()
    sys.exit(0 if success else 1)